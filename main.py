import torch
import torchinfo
import numpy as np
from torchvision import transforms
from argparse import ArgumentParser
from torch.utils.data import DataLoader

from src.data.transform import (
    generate_image_and_label, 
    generate_grayscale_and_label, 
    generate_signal_and_label
)
from datasets import load_dataset
from src.modeling.backbone import *
from src.data.stats import GetMeanStd
from src.utils.registry import REGISTRY
from src.config.config import get_cfg_defaults
from src.modeling.detector import VulnerabilityDetectionModel

import lightning.pytorch as pl
from lightning.pytorch.loggers.wandb import WandbLogger
from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping


IMAGENET_MEAN = [0.485, 0.456, 0.406] 
IMAGENET_STD = [0.229, 0.224, 0.225]


if __name__ == '__main__':
    args = ArgumentParser()
    args.add_argument('config_path', help='Path of the model\'s configuration file')
    args = args.parse_args()
    
    cfg = get_cfg_defaults()
    cfg.merge_from_file(args.config_path)

    model = REGISTRY[cfg.MODEL.NAME](num_classes=cfg.MODEL.N_CLASSES)
    task = VulnerabilityDetectionModel(
        model, 
        from_scratch=cfg.TRAINING.TRAIN_ALL_LAYERS, 
        lr=cfg.TRAINING.OPTIMIZER.LR, 
        weight_decay=cfg.TRAINING.OPTIMIZER.WEIGHT_DECAY)
    torchinfo.summary(task)

    train_ds = load_dataset("mwritescode/slither-audited-smart-contracts", 'big-multilabel', split='train', ignore_verifications=True)
    val_ds = load_dataset("mwritescode/slither-audited-smart-contracts", 'big-multilabel', split='validation', ignore_verifications=True)
    test_ds = load_dataset("mwritescode/slither-audited-smart-contracts", 'big-multilabel', split='test', ignore_verifications=True)

    train_ds = train_ds.filter(lambda elem: elem['bytecode'] != '0x')
    val_ds = val_ds.filter(lambda elem: elem['bytecode'] != '0x')
    test_ds = test_ds.filter(lambda elem: elem['bytecode'] != '0x')

    if cfg.DATASET.GRAYSCALE_3CH:
        map_func = generate_grayscale_and_label
    elif cfg.DATASET.RGB_IMAGES:
        map_func = generate_image_and_label
    else:
        map_func = generate_signal_and_label

    train_ds = train_ds.map(map_func, remove_columns=['address', 'source_code', 'bytecode', 'slither'])
    val_ds = val_ds.map(map_func, remove_columns=['address', 'source_code', 'bytecode', 'slither'])
    test_ds = test_ds.map(map_func, remove_columns=['address', 'source_code', 'bytecode', 'slither'])

    is_1D = (not cfg.DATASET.RGB_IMAGES) and (not cfg.DATASET.GRAYSCALE_3CH)

    if not cfg.USE_IMAGENET_STATS:
        get_stats = GetMeanStd(train_ds, batch_size=16, img_size=cfg.DATASET.IMG_SHAPE, is_1d=is_1D)
        mean, std = get_stats()
    else:
        mean, std = IMAGENET_MEAN, IMAGENET_STD

    img_transform = transforms.Compose([
            transforms.Resize(cfg.DATASET.IMG_SHAPE),
            transforms.ToTensor(),
            transforms.Normalize(mean=mean, std=std),
        ])
    
    max_len = cfg.DATASET.MAX_SEQ_LEN
    padding_val = 0 if cfg.DATASET.NORMALIZE else 256

    def img_label_to_tensor(examples):
        if 'image' in examples.keys():
            if not is_1D:
                examples['image'] = [img_transform(elem) for elem in examples['image']]
            else:
                examples['image'] = [np.pad(img, pad_width=(0, max_len - len(img)), constant_values=padding_val) if len(img) < max_len else img[:max_len] for img in examples['image']]
                if cfg.DATASET.NORMALIZE:
                    examples['image'] = [(torch.tensor(img).float() - mean)/std for img in examples['image']]
                else:
                    examples['image'] = [torch.tensor(img) for img in examples['image']]

        if 'label' in examples.keys():
            examples['label'] = torch.tensor(examples['label'])
        return examples

    train_ds.set_transform(img_label_to_tensor)
    val_ds.set_transform(img_label_to_tensor)
    test_ds.set_transform(img_label_to_tensor)

    batch_size = cfg.DATASET.LOADER.BATCH_SIZE
    num_workers = cfg.DATASET.LOADER.NUM_WORKERS
    loader_train = DataLoader(train_ds,
                        batch_size=batch_size,
                        drop_last=True,
                        shuffle=True,
                        pin_memory=True,
                        num_workers=num_workers)
    loader_val = DataLoader(val_ds,
                        batch_size=batch_size,
                        drop_last=False,
                        shuffle=False, 
                        pin_memory=True,
                        num_workers=num_workers)
    loader_test = DataLoader(test_ds,
                        batch_size=batch_size,
                        drop_last=False,
                        shuffle=False, 
                        pin_memory=True,
                        num_workers=num_workers)
    
    logger = WandbLogger(project='vulnerability-detection', log_model='all', name=cfg.TRAINING.LOGGER.RUN_TAG)
    callbacks = [ModelCheckpoint(monitor=cfg.TRAINING.CHECKPOINTS.MONITOR, mode=cfg.TRAINING.CHECKPOINTS.MODE)]

    if cfg.TRAINING.EARLY_STOPPING.USE:
        es_callback = EarlyStopping(monitor=cfg.TRAINING.EARLY_STOPPING.MONITOR, mode=cfg.TRAINING.EARLY_STOPPING.MODE)
        callbacks += [es_callback]

    trainer = pl.Trainer(logger=logger, gradient_clip_val=1.0, check_val_every_n_epoch=1, accumulate_grad_batches=8, callbacks=callbacks, accelerator="gpu", devices=1, max_epochs=cfg.TRAINING.N_EPOCHS)
    trainer.fit(model=task, train_dataloaders=loader_train, val_dataloaders=loader_val)
    trainer.test(ckpt_path="best", dataloaders=loader_test)